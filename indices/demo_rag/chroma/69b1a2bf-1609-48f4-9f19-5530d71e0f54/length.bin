The embedding layer uses state-of-the-art models from sentence-transformers. By default, it uses all-MiniLM-L6-v2, a lightweight 80MB model with 384 dimensions. These models convert text into high-dimensional vectors that capture semantic meaning. The embeddings are then stored in efficient vector databases like ChromaDB and FAISS. Users can easily swap embedding models through YAML configuration 