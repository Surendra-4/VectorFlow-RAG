name: Tests & Quality Checks

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']
    
    steps:
    # Step 1: Check out code
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for better analysis
    
    # Step 2: Set up Python
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    
    # Step 3: Display Python version
    - name: Display Python version
      run: python -c "import sys; print(sys.version)"
    
    # Step 4: Install dependencies
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-xdist flake8 black isort mypy
    
    # Step 5: Code formatting check (Black)
    - name: Check code formatting with Black
      run: black --check src/ tests/ --verbose
      continue-on-error: true
    
    # Step 6: Import sorting check (isort)
    - name: Check import sorting with isort
      run: isort --check-only src/ tests/ --verbose
      continue-on-error: true
    
    # Step 7: Linting (Flake8)
    - name: Lint with flake8
      run: |
        # Stop the build if there are Python syntax errors or undefined names
        flake8 src/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
        # Exit-code is 0 even if there are errors
        flake8 src/ tests/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
      continue-on-error: true
    
    # Step 8: Type checking (mypy)
    - name: Type checking with mypy
      run: mypy src/ --ignore-missing-imports --allow-untyped-defs
      continue-on-error: true
    
    # Step 9: Run unit tests with pytest
    - name: Run unit tests with pytest
      run: pytest tests/ -v --cov=src --cov-report=xml --cov-report=html
      timeout-minutes: 30
    
    # Step 10: Upload coverage reports
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false
    
    # Step 11: Archive test results
    - name: Archive test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-results-${{ matrix.python-version }}
        path: |
          htmlcov/
          coverage.xml
        retention-days: 30

  # Benchmark job (runs on push to main only)
  benchmark:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run MS MARCO benchmark
      run: python experiments/benchmark_ms_marco.py --alpha 0.5 --limit 30
      timeout-minutes: 60
      continue-on-error: true
    
    - name: Run embedding ablation
      run: python experiments/embedding_ablation.py --num_docs 50
      timeout-minutes: 60
      continue-on-error: true
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: experiments/*.json
        retention-days: 90

  # Code quality summary
  quality:
    runs-on: ubuntu-latest
    needs: test
    if: always()
    
    steps:
    - name: Check test status
      run: |
        if [ "${{ needs.test.result }}" == "failure" ]; then
          echo "::warning::Tests failed"
          exit 1
        fi
        echo "âœ“ All quality checks passed"
