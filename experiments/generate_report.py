# C:\Users\use\OneDrive\Desktop\VectorFlow-RAG\experiments\generate_report.py

"""
Generate comprehensive benchmark report
"""
import json
from datetime import datetime
from pathlib import Path

def generate_report(results_file: str = "experiments/ms_marco_results.json",
                   output_file: str = "experiments/BENCHMARK_REPORT.md"):
    """Generate markdown report from benchmark results"""
    
    with open(results_file, 'r') as f:
        results = json.load(f)
    
    metrics = results['metrics']
    config = results['config']
    
    report = f"""# VectorFlow-RAG Benchmark Report

**Generated**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## Configuration

- **Hybrid Search Alpha**: {config['alpha']} (0=pure BM25, 1=pure vector)
- **LLM Model**: {config['llm_model']}
- **Evaluation Dataset**: Mock MS MARCO-like dataset

## Results Summary

### Ranking Metrics

| Metric | Score | Interpretation |
|--------|-------|---|
| **MRR@10** | {metrics['MRR@10']:.4f} | Rank of first relevant result |
| **NDCG@10** | {metrics['NDCG@10']:.4f} | Quality of ranking (graded relevance) |

### Recall Metrics

| Metric | Score |
|--------|-------|
| Recall@5 | {metrics['Recall@5']:.4f} |
| Recall@10 | {metrics['Recall@10']:.4f} |
| Recall@20 | {metrics['Recall@20']:.4f} |
| Recall@50 | {metrics['Recall@50']:.4f} |
| Recall@100 | {metrics['Recall@100']:.4f} |

### Latency Metrics

| Metric | Value |
|--------|-------|
| Mean Latency | {metrics['Latency_mean_ms']:.2f} ms |
| P95 Latency | {metrics['Latency_p95_ms']:.2f} ms |
| P99 Latency | {metrics['Latency_p99_ms']:.2f} ms |

## Interpretation

### What These Metrics Mean

- **MRR@10**: How quickly we find the first relevant result (1.0 = best, 0.0 = worst)
- **NDCG@10**: Overall ranking quality considering position and relevance
- **Recall@k**: What percentage of all relevant results are found in top-k
- **Latency**: How fast queries are processed (lower is better)

### Performance Assessment

"""
    
    # Add performance assessment
    if metrics['NDCG@10'] > 0.7:
        report += "**✅ Excellent NDCG** - Ranking quality is very good\n\n"
    elif metrics['NDCG@10'] > 0.5:
        report += "**⚠️ Good NDCG** - Ranking quality is decent\n\n"
    else:
        report += "**❌ Low NDCG** - Consider tuning alpha or embedding model\n\n"
    
    if metrics['Recall@10'] > 0.8:
        report += "**✅ Excellent Recall@10** - Most relevant documents are found\n\n"
    elif metrics['Recall@10'] > 0.6:
        report += "**⚠️ Good Recall@10** - Most relevant documents are found\n\n"
    else:
        report += "**❌ Low Recall@10** - Missing relevant documents\n\n"
    
    if metrics['Latency_mean_ms'] < 100:
        report += "**✅ Fast Latency** - Sub-100ms response time is excellent\n\n"
    elif metrics['Latency_mean_ms'] < 500:
        report += "**⚠️ Acceptable Latency** - Under 500ms is acceptable for most use cases\n\n"
    else:
        report += "**❌ Slow Latency** - Consider optimizing indexing\n\n"
    
    report += """## Recommendations

### If NDCG is Low:
1. **Adjust alpha**: Try alpha=0.3 (more BM25) for keyword queries
2. **Try better embedding**: Switch to `all-mpnet-base-v2` or `BAAI/bge-large-en-v1.5`
3. **Tune chunking**: Experiment with different chunk sizes

### If Recall is Low:
1. **Increase k in retrieval**: Retrieve more candidates before reranking
2. **Lower alpha**: BM25 helps with exact matches
3. **Check corpus quality**: Ensure passages contain relevant information

### If Latency is High:
1. **Use lighter embedding**: `all-MiniLM-L6-v2` is faster than `all-mpnet-base-v2`
2. **Batch queries**: Process multiple queries together
3. **Index optimization**: Use smaller chunks for faster processing

## Next Steps

- [ ] Run benchmark on full MS MARCO dataset
- [ ] A/B test different embedding models
- [ ] Compare with baseline systems (BM25-only, vector-only)
- [ ] Test on domain-specific datasets

---

*Report generated by VectorFlow-RAG Benchmark Suite*
"""
    
    # Save report
    with open(output_file, 'w') as f:
        f.write(report)
    
    print(f"✓ Report saved to {output_file}")
    print("\nReport Preview:")
    print("="*80)
    print(report)
    print("="*80)

if __name__ == "__main__":
    generate_report()
